{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3b6f573-4164-426e-a231-b7a82b3d56b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import time\n",
    "import sys\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from io import StringIO\n",
    "import io\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae77970a-0601-4ea3-95a0-e780ae8db0e6",
   "metadata": {},
   "source": [
    "## Data Scrape Index: (1950-Current Season)\n",
    "\n",
    "### All data was scraped from basketballreference.com\n",
    "https://www.basketball-reference.com/ \n",
    "\n",
    "##### Section 1 | Player Data: (6 csv's)\n",
    "- RS | Per Game | Player\n",
    "- RS | Advanced | Player\n",
    "- RS | Totals | Player\n",
    "- PO | Per Game | Player\n",
    "- PO | Advanced | Player\n",
    "- PO | Totals | Player\n",
    "\n",
    "- Time: 38 minutes\n",
    "\n",
    "##### Section 2 | All Awards Voting/Teams (Player/Coaches): (2 csv's)\n",
    "- MVP voting | Player\n",
    "- ROY voting | Player\n",
    "- DPOY voting | Player\n",
    "- SMOY voting | Player\n",
    "- MIP voting | Player\n",
    "- CPOY voting | Player\n",
    "- All-NBA | Player\n",
    "- All-Defense | Player\n",
    "- All-Rookie | Player\n",
    "- COY voting | Coach\n",
    "\n",
    "- Time: 366 minutes\n",
    "\n",
    "##### Section 3 | Team Data: (6 csv's)\n",
    "- RS | Per Game | Team\n",
    "- RS | Opp Per Game | Team\n",
    "- RS | Advanced | Team\n",
    "- PO | Advaned | Team\n",
    "- RS | Schedule/Results | Team (play-in games NOT counted)\n",
    "- PO | Schedule/Results | Team (play-in games NOT counted)\n",
    "\n",
    "- Time: 38 minutes\n",
    "\n",
    "##### Section 4  (Team and Coaches): (3 csv's)\n",
    "- RS | Pre-Season Odds | Team\n",
    "- RS | Expanded Standings | Team\n",
    "- Season | Coaches\n",
    "\n",
    "- Time: 60 minutes\n",
    "\n",
    "##### Team/Player Indexs | NOT SCRAPED | Needed to be updated Yearly.\n",
    "- custom_team_season_index | contains data relating to each seasons team, conference, playoff_seed, playoff matchups, team_id, league, and team abbreviations.\n",
    "- custom_team_franchise_index | contains data unique to every NBA Franchise ever (has id for each team, think about it if a team change names its still the franchise, example Seattle SuperSonics and Oklahoma City Thunder have the same team id.\n",
    "\n",
    "##### Total Files and Scrape:\n",
    "- 17 scrape files\n",
    "- 2 index manually updation files\n",
    "- total files 19\n",
    "- total scrape time = 8.37 hours or 502 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc8c957c-0572-4446-82de-5bbdc5eda599",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#set year range\n",
    "\n",
    "start = 1950 #\n",
    "end = 2025 #minus 1, so if you type 2025, it actually means = 2024\n",
    "\n",
    "seasons_list = [str(year) for year in range(start, end)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99406df-6349-4f93-83d7-11656b7e9b3c",
   "metadata": {},
   "source": [
    "### RS | Per Game | Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "535e8631-7a03-4e71-93ed-bca7783603b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: [##################################################] 100.00%"
     ]
    }
   ],
   "source": [
    "\n",
    "def scrape_season(season):\n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    url = f'https://www.basketball-reference.com/leagues/NBA_{season}_per_game.html'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        table = soup.find('table', {'id': 'per_game_stats'})\n",
    "\n",
    "        if table:\n",
    "            html_str = str(table)\n",
    "            df = pd.read_html(StringIO(html_str))[0]\n",
    "\n",
    "            df['Season'] = season\n",
    "\n",
    "            all_data = pd.concat([all_data, df], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"No table found for {season}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for {season}\")\n",
    "        return None\n",
    "\n",
    "    time.sleep(4)\n",
    "\n",
    "    return all_data\n",
    "\n",
    "def scrape_all_seasons(seasons):\n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    total_seasons = len(seasons)\n",
    "\n",
    "    for i, season in enumerate(reversed(seasons), 1):\n",
    "        data = scrape_season(season)\n",
    "\n",
    "        if data is not None:\n",
    "            all_data = pd.concat([all_data, data], ignore_index=True)\n",
    "\n",
    "        completion_percentage = (i / total_seasons) * 100\n",
    "        sys.stdout.write(f\"\\rScraping: [{'#' * int(completion_percentage // 2)}{' ' * (50 - int(completion_percentage // 2))}] {completion_percentage:.2f}%\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    all_data = all_data[['Season'] + [col for col in all_data.columns if col != 'Season']]\n",
    "    all_data = all_data[~all_data['Player'].isin(all_data.columns)]\n",
    "\n",
    "    return all_data\n",
    "\n",
    "seasons_list = [str(year) for year in range(start, end)]\n",
    "\n",
    "result = scrape_all_seasons(seasons_list)\n",
    "\n",
    "result['Player'] = result['Player'].str.replace('*', '')\n",
    "\n",
    "RS_Per_Game_Player = result\n",
    "\n",
    "#save\n",
    "RS_Per_Game_Player.to_csv('RS_Per_Game_Player.csv', index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc62a8e5-6f93-4490-beb8-60967e113163",
   "metadata": {},
   "source": [
    "### RS | Advanced | Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "86ba9ba5-03cc-4be0-9abe-af32d92b0a17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: [##################################################] 100.00%"
     ]
    }
   ],
   "source": [
    "\n",
    "def scrape_season(season):\n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    url = f'https://www.basketball-reference.com/leagues/NBA_{season}_advanced.html'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        table = soup.find('table', {'id': 'advanced_stats'})\n",
    "\n",
    "        if table:\n",
    "            html_str = str(table)\n",
    "            df = pd.read_html(StringIO(html_str))[0]\n",
    "\n",
    "            df['Season'] = season\n",
    "\n",
    "            all_data = pd.concat([all_data, df], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"No table found for {season}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for {season}\")\n",
    "        return None\n",
    "\n",
    "    time.sleep(4)\n",
    "\n",
    "    return all_data\n",
    "\n",
    "def scrape_all_seasons(seasons):\n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    total_seasons = len(seasons)\n",
    "\n",
    "    for i, season in enumerate(reversed(seasons), 1):\n",
    "        data = scrape_season(season)\n",
    "\n",
    "        if data is not None:\n",
    "            all_data = pd.concat([all_data, data], ignore_index=True)\n",
    "\n",
    "        completion_percentage = (i / total_seasons) * 100\n",
    "        sys.stdout.write(f\"\\rScraping: [{'#' * int(completion_percentage // 2)}{' ' * (50 - int(completion_percentage // 2))}] {completion_percentage:.2f}%\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    all_data = all_data[['Season'] + [col for col in all_data.columns if col != 'Season']]\n",
    "    all_data = all_data[['Season'] + [col for col in all_data.columns if col not in ['Season']]]\n",
    "    all_data = all_data[~all_data['Player'].isin(all_data.columns)]\n",
    "\n",
    "    return all_data\n",
    "\n",
    "seasons_list = [str(year) for year in range(start, end)]\n",
    "\n",
    "result = scrape_all_seasons(seasons_list)\n",
    "\n",
    "result = result.loc[:, ~result.columns.str.contains('^Unnamed')]\n",
    "\n",
    "result['Player'] = result['Player'].str.replace('*', '')\n",
    "\n",
    "RS_Advanced_Player = result\n",
    "\n",
    "#save\n",
    "RS_Advanced_Player.to_csv('RS_Advanced_Player.csv', index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63f3ea7-5460-4dcc-9f85-ae14487cda2a",
   "metadata": {},
   "source": [
    "### RS | Totals | Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "95c19978-e01c-45ba-9e3c-fb270598ff3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: [##################################################] 100.00%"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def scrape_season(season):\n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    url = f'https://www.basketball-reference.com/leagues/NBA_{season}_totals.html'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        table = soup.find('table', {'id': 'totals_stats'})\n",
    "\n",
    "        if table:\n",
    "            html_str = str(table)\n",
    "            df = pd.read_html(StringIO(html_str))[0]\n",
    "\n",
    "            df['Season'] = season\n",
    "\n",
    "            all_data = pd.concat([all_data, df], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"No table found for {season}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for {season}\")\n",
    "        return None\n",
    "\n",
    "    time.sleep(4)\n",
    "\n",
    "    return all_data\n",
    "\n",
    "def scrape_all_seasons(seasons):\n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    total_seasons = len(seasons)\n",
    "\n",
    "    for i, season in enumerate(reversed(seasons), 1):\n",
    "        data = scrape_season(season)\n",
    "\n",
    "        if data is not None:\n",
    "            all_data = pd.concat([all_data, data], ignore_index=True)\n",
    "\n",
    "        completion_percentage = (i / total_seasons) * 100\n",
    "        sys.stdout.write(f\"\\rScraping: [{'#' * int(completion_percentage // 2)}{' ' * (50 - int(completion_percentage // 2))}] {completion_percentage:.2f}%\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    all_data = all_data[['Season'] + [col for col in all_data.columns if col != 'Season']]\n",
    "    all_data = all_data[['Season'] + [col for col in all_data.columns if col not in ['Season']]]\n",
    "    all_data = all_data[~all_data['Player'].isin(all_data.columns)]\n",
    "\n",
    "    return all_data\n",
    "\n",
    "seasons_list = [str(year) for year in range(start, end)]\n",
    "\n",
    "result = scrape_all_seasons(seasons_list)\n",
    "\n",
    "result = result.loc[:, ~result.columns.str.contains('^Unnamed')]\n",
    "\n",
    "result['Player'] = result['Player'].str.replace('*', '')\n",
    "\n",
    "RS_Totals_Player = result\n",
    "\n",
    "#save\n",
    "RS_Totals_Player.to_csv('RS_Totals_Player.csv', index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f69728-ce80-4684-b93c-6e6b58151b25",
   "metadata": {},
   "source": [
    "### PO | Per Game | Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9ba7e400-86fc-4ff2-a58a-013c9a712500",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: [##################################################] 100.00%"
     ]
    }
   ],
   "source": [
    "def scrape_season(season):\n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    url = f'https://www.basketball-reference.com/playoffs/NBA_{season}_per_game.html'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        table = soup.find('table', {'id': 'per_game_stats'})\n",
    "\n",
    "        if table:\n",
    "            html_str = str(table)\n",
    "            df = pd.read_html(StringIO(html_str))[0]\n",
    "\n",
    "            df['Season'] = season\n",
    "\n",
    "            all_data = pd.concat([all_data, df], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"No table found for {season}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for {season}\")\n",
    "        return None\n",
    "\n",
    "    time.sleep(4)\n",
    "\n",
    "    return all_data\n",
    "\n",
    "def scrape_all_seasons(seasons):\n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    total_seasons = len(seasons)\n",
    "\n",
    "    for i, season in enumerate(reversed(seasons), 1):\n",
    "        data = scrape_season(season)\n",
    "\n",
    "        if data is not None:\n",
    "            all_data = pd.concat([all_data, data], ignore_index=True)\n",
    "\n",
    "        completion_percentage = (i / total_seasons) * 100\n",
    "        sys.stdout.write(f\"\\rScraping: [{'#' * int(completion_percentage // 2)}{' ' * (50 - int(completion_percentage // 2))}] {completion_percentage:.2f}%\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    all_data = all_data[['Season'] + [col for col in all_data.columns if col != 'Season']]\n",
    "    all_data = all_data[~all_data['Player'].isin(all_data.columns)]\n",
    "\n",
    "    return all_data\n",
    "\n",
    "seasons_list = [str(year) for year in range(start, end)]\n",
    "\n",
    "result = scrape_all_seasons(seasons_list)\n",
    "\n",
    "result['Player'] = result['Player'].str.replace('*', '')\n",
    "\n",
    "PO_Per_Game_Player = result\n",
    "\n",
    "#save\n",
    "PO_Per_Game_Player.to_csv('PO_Per_Game_Player.csv', index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c048946-4230-4906-857c-a05d120f97da",
   "metadata": {},
   "source": [
    "### PO | Advanced | Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "06cba763-30f7-4e5c-9d6e-332563f96d62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: [##################################################] 100.00%"
     ]
    }
   ],
   "source": [
    "def scrape_season(season):\n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    url = f'https://www.basketball-reference.com/playoffs/NBA_{season}_advanced.html'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        table = soup.find('table', {'id': 'advanced_stats'})\n",
    "\n",
    "        if table:\n",
    "            html_str = str(table)\n",
    "            df = pd.read_html(StringIO(html_str))[0]\n",
    "\n",
    "            df['Season'] = season\n",
    "\n",
    "            all_data = pd.concat([all_data, df], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"No table found for {season}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for {season}\")\n",
    "        return None\n",
    "\n",
    "    time.sleep(4)\n",
    "\n",
    "    return all_data\n",
    "\n",
    "def scrape_all_seasons(seasons):\n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    total_seasons = len(seasons)\n",
    "\n",
    "    for i, season in enumerate(reversed(seasons), 1):\n",
    "        data = scrape_season(season)\n",
    "\n",
    "        if data is not None:\n",
    "            all_data = pd.concat([all_data, data], ignore_index=True)\n",
    "\n",
    "        completion_percentage = (i / total_seasons) * 100\n",
    "        sys.stdout.write(f\"\\rScraping: [{'#' * int(completion_percentage // 2)}{' ' * (50 - int(completion_percentage // 2))}] {completion_percentage:.2f}%\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    all_data = all_data[['Season'] + [col for col in all_data.columns if col != 'Season']]\n",
    "    all_data = all_data[~all_data['Player'].isin(all_data.columns)]\n",
    "\n",
    "    return all_data\n",
    "\n",
    "seasons_list = [str(year) for year in range(start, end)]\n",
    "\n",
    "result = scrape_all_seasons(seasons_list)\n",
    "\n",
    "result = result.loc[:, ~result.columns.str.contains('^Unnamed')]\n",
    "\n",
    "result['Player'] = result['Player'].str.replace('*', '')\n",
    "\n",
    "PO_Advanced_Player = result\n",
    "\n",
    "#save\n",
    "PO_Advanced_Player.to_csv('PO_Advanced_Player.csv', index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eb3284-169f-4a6f-a1a6-6736db859571",
   "metadata": {},
   "source": [
    "### PO | Totals | Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e7e2f609-28e1-41c8-9562-8927c25d4734",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: [##################################################] 100.00%"
     ]
    }
   ],
   "source": [
    "\n",
    "def scrape_season(season):\n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    url = f'https://www.basketball-reference.com/playoffs/NBA_{season}_totals.html'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        table = soup.find('table', {'id': 'totals_stats'})\n",
    "\n",
    "        if table:\n",
    "            html_str = str(table)\n",
    "            df = pd.read_html(StringIO(html_str))[0]\n",
    "\n",
    "            df['Season'] = season\n",
    "\n",
    "            all_data = pd.concat([all_data, df], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"No table found for {season}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for {season}\")\n",
    "        return None\n",
    "\n",
    "    time.sleep(4)\n",
    "\n",
    "    return all_data\n",
    "\n",
    "def scrape_all_seasons(seasons):\n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    total_seasons = len(seasons)\n",
    "\n",
    "    for i, season in enumerate(reversed(seasons), 1):\n",
    "        data = scrape_season(season)\n",
    "\n",
    "        if data is not None:\n",
    "            all_data = pd.concat([all_data, data], ignore_index=True)\n",
    "\n",
    "        completion_percentage = (i / total_seasons) * 100\n",
    "        sys.stdout.write(f\"\\rScraping: [{'#' * int(completion_percentage // 2)}{' ' * (50 - int(completion_percentage // 2))}] {completion_percentage:.2f}%\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    all_data = all_data[['Season'] + [col for col in all_data.columns if col != 'Season']]\n",
    "    all_data = all_data[['Season'] + [col for col in all_data.columns if col not in ['Season']]]\n",
    "    all_data = all_data[~all_data['Player'].isin(all_data.columns)]\n",
    "\n",
    "    return all_data\n",
    "\n",
    "seasons_list = [str(year) for year in range(start, end)]\n",
    "\n",
    "result = scrape_all_seasons(seasons_list)\n",
    "\n",
    "result = result.loc[:, ~result.columns.str.contains('^Unnamed')]\n",
    "\n",
    "result['Player'] = result['Player'].str.replace('*', '')\n",
    "\n",
    "PO_Totals_Player = result\n",
    "\n",
    "#save\n",
    "PO_Totals_Player.to_csv('PO_Totals_Player.csv', index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3140df1d-c9f3-4d9d-bfb0-27d3e497c055",
   "metadata": {},
   "source": [
    "### All Award Voting | Player | Coaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "af395cfb-a7b2-4786-8661-ca1a730c2572",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: [##################################################] 100.00%Columns before removing 'Unnamed': Index(['Rank', 'Player', 'Age', 'Tm', 'First', 'Pts Won', 'Pts Max', 'Share',\n",
      "       'G', 'MP', 'PTS', 'TRB', 'AST', 'STL', 'BLK', 'FG%', '3P%', 'FT%', 'WS',\n",
      "       'WS/48', 'Season', 'award_type', 'DWS', 'DBPM', 'DRtg', '# Tm', 'Pos',\n",
      "       '1st Tm', '2nd Tm', '3rd Tm', 'Coach', 'W', 'L', 'W/L%'],\n",
      "      dtype='object')\n",
      "Columns after removing 'Unnamed': Index(['Rank', 'Player', 'Age', 'Tm', 'First', 'Pts Won', 'Pts Max', 'Share',\n",
      "       'G', 'MP', 'PTS', 'TRB', 'AST', 'STL', 'BLK', 'FG%', '3P%', 'FT%', 'WS',\n",
      "       'WS/48', 'Season', 'award_type', 'DWS', 'DBPM', 'DRtg', '# Tm', 'Pos',\n",
      "       '1st Tm', '2nd Tm', '3rd Tm', 'Coach', 'W', 'L', 'W/L%'],\n",
      "      dtype='object')\n",
      "Columns after flattening MultiIndex: Index(['Rank', 'Player', 'Age', 'Tm', 'First', 'Pts Won', 'Pts Max', 'Share',\n",
      "       'G', 'MP', 'PTS', 'TRB', 'AST', 'STL', 'BLK', 'FG%', '3P%', 'FT%', 'WS',\n",
      "       'WS/48', 'Season', 'award_type', 'DWS', 'DBPM', 'DRtg', '# Tm', 'Pos',\n",
      "       '1st Tm', '2nd Tm', '3rd Tm', 'Coach', 'W', 'L', 'W/L%'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def scrape_season_table(season, table_id):\n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    url = f'https://www.basketball-reference.com/awards/awards_{season}.html'\n",
    "\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')  \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(url)\n",
    "\n",
    "    try:\n",
    "        table = driver.find_element(By.ID, table_id)\n",
    "        table_html = table.get_attribute('outerHTML')\n",
    "        driver.quit()\n",
    "\n",
    "        df = pd.read_html(StringIO(table_html), header=[1])[0]\n",
    "\n",
    "        df['Season'] = season\n",
    "        df['award_type'] = table_id\n",
    "\n",
    "        all_data = pd.concat([all_data, df], ignore_index=True)\n",
    "    except Exception as e:\n",
    "        print(f\"No table found for {table_id} in {season}\")\n",
    "        print(e)\n",
    "        driver.quit()\n",
    "        return None\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "    return all_data\n",
    "\n",
    "def scrape_all_seasons_tables(seasons, table_ids):\n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    total_tasks = len(seasons) * len(table_ids)\n",
    "    task_count = 0\n",
    "\n",
    "    for season in reversed(seasons):\n",
    "        for table_id in table_ids:\n",
    "            data = scrape_season_table(season, table_id)\n",
    "\n",
    "            if data is not None:\n",
    "                all_data = pd.concat([all_data, data], ignore_index=True)\n",
    "\n",
    "            task_count += 1\n",
    "            completion_percentage = (task_count / total_tasks) * 100\n",
    "            sys.stdout.write(f\"\\rScraping: [{'#' * int(completion_percentage // 2)}{' ' * (50 - int(completion_percentage // 2))}] {completion_percentage:.2f}%\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    print(\"Columns before removing 'Unnamed':\", all_data.columns)\n",
    "\n",
    "    all_data.columns = all_data.columns.map(str)\n",
    "    all_data = all_data.loc[:, ~all_data.columns.str.contains('^Unnamed')]\n",
    "\n",
    "    print(\"Columns after removing 'Unnamed':\", all_data.columns)\n",
    "\n",
    "    if isinstance(all_data.columns, pd.MultiIndex):\n",
    "        all_data.columns = [' '.join(col).strip() for col in all_data.columns.values]\n",
    "\n",
    "    print(\"Columns after flattening MultiIndex:\", all_data.columns)\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "seasons_list = [str(year) for year in range(start, end)]\n",
    "table_ids = ['mvp', 'roy', 'dpoy', 'smoy', 'mip', 'clutch_poy', 'leading_all_nba', 'leading_all_defense', 'leading_all_rookie', 'coy']\n",
    "\n",
    "result = scrape_all_seasons_tables(seasons_list, table_ids)\n",
    "\n",
    "all_award_voting = result\n",
    "\n",
    "#save\n",
    "all_award_voting.to_csv('all_award_voting.csv', index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5e219745-4241-4d3c-85ca-4f6d49beff9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cleaning all award data\n",
    "\n",
    "coach_data = all_award_voting[all_award_voting['award_type'] == 'coy']\n",
    "all_award_voting = all_award_voting[all_award_voting['award_type'] != 'coy']\n",
    "all_award_voting = all_award_voting[all_award_voting['Player'].notna() & (all_award_voting['Player'] != '')]\n",
    "\n",
    "all_award_voting['1st_team'] = all_award_voting['# Tm'].apply(lambda x: 1 if x in ['1st', '1T'] else 0)\n",
    "all_award_voting['2nd_team'] = all_award_voting['# Tm'].apply(lambda x: 1 if x in ['2nd', '2T'] else 0)\n",
    "all_award_voting['3rd_team'] = all_award_voting['# Tm'].apply(lambda x: 1 if x in ['3rd', '3T'] else 0)\n",
    "\n",
    "all_award_voting = all_award_voting[['Rank', 'Player', 'Tm', 'Share', 'Season', 'award_type', '1st_team', '2nd_team', '3rd_team']]\n",
    "\n",
    "coach_data = coach_data.rename(columns={'Share': 'coy_share'})\n",
    "coach_data = coach_data[['Rank', 'Coach', 'Tm', 'coy_share', 'Season']]\n",
    "\n",
    "all_award_voting['mvp_share'] = all_award_voting.apply(lambda x: x['Share'] if x['award_type'] == 'mvp' else None, axis=1)\n",
    "all_award_voting['dpoy_share'] = all_award_voting.apply(lambda x: x['Share'] if x['award_type'] == 'dpoy' else None, axis=1)\n",
    "all_award_voting['roy_share'] = all_award_voting.apply(lambda x: x['Share'] if x['award_type'] == 'roy' else None, axis=1)\n",
    "all_award_voting['smoy_share'] = all_award_voting.apply(lambda x: x['Share'] if x['award_type'] == 'smoy' else None, axis=1)\n",
    "all_award_voting['mip_share'] = all_award_voting.apply(lambda x: x['Share'] if x['award_type'] == 'mip' else None, axis=1)\n",
    "all_award_voting['cpoy_share'] = all_award_voting.apply(lambda x: x['Share'] if x['award_type'] == 'clutch_poy' else None, axis=1)\n",
    "\n",
    "share_columns = ['mvp_share', 'dpoy_share', 'roy_share', 'smoy_share', 'mip_share', 'cpoy_share']\n",
    "all_award_voting[share_columns] = all_award_voting[share_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "all_award_voting['leading_all_nba'] = all_award_voting['award_type'].apply(lambda x: 1 if x == 'leading_all_nba' else 0)\n",
    "all_award_voting['leading_all_defense'] = all_award_voting['award_type'].apply(lambda x: 1 if x == 'leading_all_defense' else 0)\n",
    "all_award_voting['leading_all_rookie'] = all_award_voting['award_type'].apply(lambda x: 1 if x == 'leading_all_rookie' else 0)\n",
    "\n",
    "# Get unique award types\n",
    "award_types = all_award_voting['award_type'].unique()\n",
    "\n",
    "# Create new columns for each unique award_type combined with '1st_team', '2nd_team', '3rd_team'\n",
    "for award_type in award_types:\n",
    "    all_award_voting[f'{award_type}_1st_team'] = all_award_voting.apply(\n",
    "        lambda row: row['1st_team'] if row['award_type'] == award_type else 0, axis=1\n",
    "    )\n",
    "    all_award_voting[f'{award_type}_2nd_team'] = all_award_voting.apply(\n",
    "        lambda row: row['2nd_team'] if row['award_type'] == award_type else 0, axis=1\n",
    "    )\n",
    "    all_award_voting[f'{award_type}_3rd_team'] = all_award_voting.apply(\n",
    "        lambda row: row['3rd_team'] if row['award_type'] == award_type else 0, axis=1\n",
    "    )\n",
    "    all_award_voting[f'{award_type}'] = all_award_voting.apply(\n",
    "        lambda row: 1 if row['award_type'] == award_type else 0, axis=1\n",
    "    )\n",
    "\n",
    "# Create 'count_all_nba', 'count_all_defense', and 'count_all_rookie' columns\n",
    "all_award_voting['count_all_nba'] = all_award_voting.apply(\n",
    "    lambda row: 1 if row['leading_all_nba_1st_team'] == 1 or row['leading_all_nba_2nd_team'] == 1 or row['leading_all_nba_3rd_team'] == 1 else 0, axis=1\n",
    ")\n",
    "all_award_voting['count_all_defense'] = all_award_voting.apply(\n",
    "    lambda row: 1 if row['leading_all_defense_1st_team'] == 1 or row['leading_all_defense_2nd_team'] == 1 else 0, axis=1\n",
    ")\n",
    "all_award_voting['count_all_rookie'] = all_award_voting.apply(\n",
    "    lambda row: 1 if row['leading_all_rookie_1st_team'] == 1 or row['leading_all_rookie_2nd_team'] == 1 else 0, axis=1\n",
    ")\n",
    "\n",
    "# Drop the specified columns\n",
    "columns_to_drop = [\n",
    "    'leading_all_defense_3rd_team', 'leading_all_rookie_3rd_team', 'leading_all_nba', 'leading_all_defense', 'leading_all_rookie',\n",
    "    'mvp_1st_team', 'mvp_2nd_team', 'mvp_3rd_team', 'mvp', 'roy_1st_team', 'roy_2nd_team', 'roy_3rd_team', 'roy',\n",
    "    'dpoy_1st_team', 'dpoy_2nd_team', 'dpoy_3rd_team', 'dpoy', 'smoy_1st_team', 'smoy_2nd_team', 'smoy_3rd_team', 'smoy',\n",
    "    'mip_1st_team', 'mip_2nd_team', 'mip_3rd_team', 'mip', 'clutch_poy_1st_team', 'clutch_poy_2nd_team', 'clutch_poy_3rd_team', 'clutch_poy'\n",
    "]\n",
    "all_award_voting.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Drop the 'Share' and 'award_type' columns after creating the new columns\n",
    "all_award_voting = all_award_voting.drop(columns=['Share', 'award_type'])\n",
    "\n",
    "all_award_voting['won_mvp'] = all_award_voting.groupby('Season')['mvp_share'].transform(lambda x: (x == x.max()).astype(int))\n",
    "all_award_voting['won_roy'] = all_award_voting.groupby('Season')['roy_share'].transform(lambda x: (x == x.max()).astype(int))\n",
    "all_award_voting['won_dpoy'] = all_award_voting.groupby('Season')['dpoy_share'].transform(lambda x: (x == x.max()).astype(int))\n",
    "all_award_voting['won_smoy'] = all_award_voting.groupby('Season')['smoy_share'].transform(lambda x: (x == x.max()).astype(int))\n",
    "all_award_voting['won_mip'] = all_award_voting.groupby('Season')['mip_share'].transform(lambda x: (x == x.max()).astype(int))\n",
    "all_award_voting['won_cpoy'] = all_award_voting.groupby('Season')['cpoy_share'].transform(lambda x: (x == x.max()).astype(int))\n",
    "\n",
    "coach_data['coy_share'] = pd.to_numeric(coach_data['coy_share'], errors='coerce')\n",
    "coach_data['won_coy'] = coach_data.groupby('Season')['coy_share'].transform(lambda x: (x == x.max()).astype(int))\n",
    "\n",
    "all_award_coach_voting = coach_data\n",
    "\n",
    "#save\n",
    "all_award_voting.to_csv('all_award_voting.csv', index=False, encoding=\"utf-8-sig\")\n",
    "all_award_coach_voting.to_csv('all_award_coach_voting.csv', index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f1154f-37e6-40c9-ad46-2c39c9cb9ff4",
   "metadata": {},
   "source": [
    "### RS | Per Game | Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9829b6d9-02b5-43c3-8da1-6b9b28c33dcc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: [##################################################] 100.00%"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def scrape_season(season):\n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    url = f'https://www.basketball-reference.com/leagues/NBA_{season}.html'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        table = soup.find('table', {'id': 'per_game-team'})\n",
    "\n",
    "        if table:\n",
    "            table_html = str(table)\n",
    "            df = pd.read_html(StringIO(table_html))[0]\n",
    "\n",
    "            df['Season'] = season\n",
    "\n",
    "            all_data = pd.concat([all_data, df], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"No table found for {season}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for {season}\")\n",
    "        return None\n",
    "\n",
    "    time.sleep(4)\n",
    "\n",
    "    return all_data\n",
    "\n",
    "def scrape_all_seasons(seasons):\n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    total_seasons = len(seasons)\n",
    "\n",
    "    for i, season in enumerate(reversed(seasons), 1):\n",
    "        data = scrape_season(season)\n",
    "\n",
    "        if data is not None:\n",
    "            all_data = pd.concat([all_data, data], ignore_index=True)\n",
    "\n",
    "        completion_percentage = (i / total_seasons) * 100\n",
    "        sys.stdout.write(f\"\\rScraping: [{'#' * int(completion_percentage // 2)}{' ' * (50 - int(completion_percentage // 2))}] {completion_percentage:.2f}%\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    all_data = all_data[~all_data['Team'].str.contains('League Average', na=False)]\n",
    "\n",
    "    all_data['make_playoffs'] = all_data['Team'].str.contains('\\*').astype(int)\n",
    "\n",
    "    all_data['Team'] = all_data['Team'].str.replace('*', '', regex=False)\n",
    "\n",
    "    all_data = all_data[['Season'] + [col for col in all_data.columns if col != 'Season']]\n",
    "    all_data = all_data[~all_data['Team'].isin(all_data.columns)]\n",
    "\n",
    "    return all_data\n",
    "\n",
    "seasons_list = [str(year) for year in range(start, end)]\n",
    "\n",
    "result = scrape_all_seasons(seasons_list)\n",
    "\n",
    "RS_Per_Game_Team = result\n",
    "\n",
    "#save\n",
    "RS_Per_Game_Team.to_csv('RS_Per_Game_Team.csv', index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0d1097-f8b4-4436-b05a-83a8b0c6c07b",
   "metadata": {},
   "source": [
    "### RS | Opp Per Game | Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "375b8802-c5f5-4b16-8ac6-2547be5095b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: [##################################################] 100.00%"
     ]
    }
   ],
   "source": [
    "def scrape_season(season):\n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    url = f'https://www.basketball-reference.com/leagues/NBA_{season}.html'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        table = soup.find('table', {'id': 'per_game-opponent'})\n",
    "\n",
    "        if table:\n",
    "            table_html = str(table)\n",
    "            df = pd.read_html(StringIO(table_html))[0]\n",
    "\n",
    "            df['Season'] = season\n",
    "\n",
    "            all_data = pd.concat([all_data, df], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"No table found for {season}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for {season}\")\n",
    "        return None\n",
    "\n",
    "    time.sleep(4)\n",
    "\n",
    "    return all_data\n",
    "\n",
    "def scrape_all_seasons(seasons):\n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    total_seasons = len(seasons)\n",
    "\n",
    "    for i, season in enumerate(reversed(seasons), 1):\n",
    "        data = scrape_season(season)\n",
    "\n",
    "        if data is not None:\n",
    "            all_data = pd.concat([all_data, data], ignore_index=True)\n",
    "\n",
    "        completion_percentage = (i / total_seasons) * 100\n",
    "        sys.stdout.write(f\"\\rScraping: [{'#' * int(completion_percentage // 2)}{' ' * (50 - int(completion_percentage // 2))}] {completion_percentage:.2f}%\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    all_data = all_data[~all_data['Team'].str.contains('League Average', na=False)]\n",
    "\n",
    "    all_data['make_playoffs'] = all_data['Team'].str.contains('\\*').astype(int)\n",
    "\n",
    "    all_data['Team'] = all_data['Team'].str.replace('*', '', regex=False)\n",
    "\n",
    "    all_data = all_data[['Season'] + [col for col in all_data.columns if col != 'Season']]\n",
    "    all_data = all_data[~all_data['Team'].isin(all_data.columns)]\n",
    "\n",
    "    return all_data\n",
    "\n",
    "seasons_list = [str(year) for year in range(start, end)]\n",
    "\n",
    "result = scrape_all_seasons(seasons_list)\n",
    "\n",
    "columns_to_exclude = ['Season', 'Rk', 'make_playoffs', 'Team']\n",
    "new_columns = {col: f'opp_{col}' for col in result.columns if col not in columns_to_exclude}\n",
    "result = result.rename(columns=new_columns)\n",
    "\n",
    "RS_Opp_Per_Game_Team = result\n",
    "\n",
    "#save\n",
    "RS_Opp_Per_Game_Team.to_csv('RS_Opp_Per_Game_Team.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b202c6b-afc4-48d9-bf02-3822e211c57c",
   "metadata": {},
   "source": [
    "### RS | Advanced | Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e0aa70a0-d5da-4100-a0d5-650c90764a4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for season 2024\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def scrape_season(season):\n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    url = f'https://www.basketball-reference.com/leagues/NBA_{season}.html'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        table = soup.find('table', {'id': 'advanced-team'})\n",
    "\n",
    "        if table:\n",
    "            html_str = str(table)\n",
    "            df = pd.read_html(StringIO(html_str))[0]\n",
    "\n",
    "            df['Season'] = season\n",
    "\n",
    "            all_data = pd.concat([all_data, df], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"No table found for {season}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for {season}\")\n",
    "        return None\n",
    "\n",
    "    time.sleep(4)\n",
    "\n",
    "    return all_data\n",
    "\n",
    "def scrape_all_seasons(seasons_list):\n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    for season in seasons_list:\n",
    "        data = scrape_season(season)\n",
    "\n",
    "        if data is not None:\n",
    "            data['Season'] = season\n",
    "\n",
    "            all_data = pd.concat([all_data, data], ignore_index=True)\n",
    "\n",
    "            print(f\"Scraping data for season {season}\")\n",
    "\n",
    "    return all_data\n",
    "\n",
    "seasons_list = [str(year) for year in range(start, end)]\n",
    "\n",
    "result = scrape_all_seasons(seasons_list)\n",
    "\n",
    "RS_Advanced_Team = result\n",
    "\n",
    "#cleaning\n",
    "RS_Advanced_Team.columns = RS_Advanced_Team.iloc[0]\n",
    "RS_Advanced_Team = RS_Advanced_Team[1:]\n",
    "RS_Advanced_Team.columns.name = None\n",
    "RS_Advanced_Team.columns = RS_Advanced_Team.columns.astype(str)\n",
    "RS_Advanced_Team = RS_Advanced_Team.loc[:, ~RS_Advanced_Team.columns.str.contains('^Unnamed')]\n",
    "\n",
    "RS_Advanced_Team = RS_Advanced_Team[RS_Advanced_Team['Team'] != 'League Average']\n",
    "RS_Advanced_Team['make_playoffs'] = RS_Advanced_Team['Team'].apply(lambda x: 1 if '*' in x else 0)\n",
    "\n",
    "RS_Advanced_Team['season'] = RS_Advanced_Team.iloc[:, RS_Advanced_Team.columns.get_loc('Attend./G') + 1].astype(str)\n",
    "RS_Advanced_Team.drop(columns=RS_Advanced_Team.columns[RS_Advanced_Team.columns.get_loc('Attend./G') + 1], inplace=True)\n",
    "\n",
    "RS_Advanced_Team['Rk'] = RS_Advanced_Team['Rk'].astype(float).round().astype(int).astype(str)\n",
    "RS_Advanced_Team['season'] = RS_Advanced_Team['season'].astype(float).round().astype(int).astype(str)\n",
    "\n",
    "RS_Advanced_Team['Team'] = RS_Advanced_Team['Team'].str.replace('*', '')\n",
    "RS_Advanced_Team['overall_record'] = RS_Advanced_Team['W'].astype(float) / (RS_Advanced_Team['W'].astype(float) + RS_Advanced_Team['L'].astype(float))\n",
    "\n",
    "RS_Advanced_Team['rk_season'] = RS_Advanced_Team.groupby('season')['overall_record'].rank(ascending=False)\n",
    "RS_Advanced_Team['rk_season'] = RS_Advanced_Team['rk_season'].fillna(0).astype(int)\n",
    "\n",
    "for idx in [17, 18, 19, 20]:\n",
    "    RS_Advanced_Team.columns.values[idx] = 'offensive_' + RS_Advanced_Team.columns[idx]\n",
    "\n",
    "for idx in [21, 22, 23, 24]:\n",
    "    RS_Advanced_Team.columns.values[idx] = 'defensive_' + RS_Advanced_Team.columns[idx]\n",
    "\n",
    "#save    \n",
    "RS_Advanced_Team.to_csv('RS_Advanced_Team.csv',index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c43ad6-eba5-4e51-8da0-d1c8185f3854",
   "metadata": {
    "tags": []
   },
   "source": [
    "### PO | Advanced | Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7dbcf2fd-737f-4112-aa7a-36ec640c3805",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for season 2024\n"
     ]
    }
   ],
   "source": [
    "def scrape_season(season):\n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    url = f'https://www.basketball-reference.com/playoffs/NBA_{season}.html'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        table = soup.find('table', {'id': 'advanced-team'})\n",
    "\n",
    "        if table:\n",
    "            html_str = str(table)\n",
    "            df = pd.read_html(StringIO(html_str))[0]\n",
    "\n",
    "            df['Season'] = season\n",
    "\n",
    "            all_data = pd.concat([all_data, df], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"No table found for {season}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for {season}\")\n",
    "        return None\n",
    "\n",
    "    time.sleep(4)\n",
    "\n",
    "    return all_data\n",
    "\n",
    "def scrape_all_seasons(seasons_list):\n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    for season in seasons_list:\n",
    "        data = scrape_season(season)\n",
    "\n",
    "        if data is not None:\n",
    "            data['Season'] = season\n",
    "\n",
    "            all_data = pd.concat([all_data, data], ignore_index=True)\n",
    "\n",
    "            print(f\"Scraping data for season {season}\")\n",
    "\n",
    "    return all_data\n",
    "\n",
    "seasons_list = [str(year) for year in range(start, end)]\n",
    "\n",
    "result = scrape_all_seasons(seasons_list)\n",
    "\n",
    "PO_Advanced_Team = result\n",
    "\n",
    "# Create a backup of the original DataFrame\n",
    "PO_Advanced_Team_backup = copy.deepcopy(PO_Advanced_Team)\n",
    "    \n",
    "# Old code\n",
    "PO_Advanced_Team.columns = PO_Advanced_Team.iloc[0]\n",
    "PO_Advanced_Team = PO_Advanced_Team[1:]\n",
    "    \n",
    "PO_Advanced_Team.columns.name = None\n",
    "    \n",
    "PO_Advanced_Team.columns = PO_Advanced_Team.columns.astype(str)\n",
    "    \n",
    "PO_Advanced_Team = PO_Advanced_Team[PO_Advanced_Team['Tm'] != 'League Average']\n",
    "    \n",
    "PO_Advanced_Team = PO_Advanced_Team.loc[:, ~PO_Advanced_Team.columns.str.contains('^Unnamed')]\n",
    "    \n",
    "PO_Advanced_Team.rename(columns={'nan': 'season'}, inplace=True)\n",
    "    \n",
    "PO_Advanced_Team['Rk'] = PO_Advanced_Team['Rk'].fillna(-1).astype(float).round().astype(int).astype(str)\n",
    "    \n",
    "PO_Advanced_Team['season'] = PO_Advanced_Team['season'].astype(float).round().astype(int).astype(str)\n",
    "    \n",
    "PO_Advanced_Team['W'] = PO_Advanced_Team['W'].astype(float)\n",
    "    \n",
    "PO_Advanced_Team['season'] = PO_Advanced_Team['season'].astype(str)\n",
    "    \n",
    "PO_Advanced_Team['champion_share'] = PO_Advanced_Team['W'] / PO_Advanced_Team.groupby('season')['W'].transform('max')\n",
    "    \n",
    "for idx in [15, 16, 17, 18]:\n",
    "    PO_Advanced_Team.columns.values[idx] = 'offensive_' + PO_Advanced_Team.columns[idx]\n",
    "    \n",
    "for idx in [19, 20, 21, 22]:\n",
    "    PO_Advanced_Team.columns.values[idx] = 'defensive_' + PO_Advanced_Team.columns[idx]\n",
    "    \n",
    "#save    \n",
    "PO_Advanced_Team.to_csv('RS_Advanced_Team.csv',index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2185effb-0e4f-4be1-8aea-b1994518c055",
   "metadata": {},
   "source": [
    "### RS | Schedule/Results | Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1693bbec-f6c0-4c2d-af7b-02bf357e26a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve data for 2024 in july\n",
      "Scraping data for season 2024\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def scrape_season(season):\n",
    "    all_data = pd.DataFrame()\n",
    "    months = ['october', 'november', 'december', 'january', 'february', 'march', 'april', 'may', 'june', 'july']\n",
    "\n",
    "    for month in months:\n",
    "        url = f'https://www.basketball-reference.com/leagues/NBA_{season}_games-{month}.html'\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            table = soup.find('table', {'id': 'schedule'})\n",
    "\n",
    "            if table:\n",
    "                html_str = str(table)\n",
    "                df = pd.read_html(StringIO(html_str))[0]\n",
    "\n",
    "                df['Season'] = season\n",
    "\n",
    "                all_data = pd.concat([all_data, df], ignore_index=True)\n",
    "            else:\n",
    "                print(f\"No table found for {season} in {month}\")\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data for {season} in {month}\")\n",
    "\n",
    "        time.sleep(4)\n",
    "\n",
    "    return all_data\n",
    "\n",
    "def scrape_all_seasons():\n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    for season in range(start, end):\n",
    "        data = scrape_season(str(season))\n",
    "\n",
    "        if data is not None:\n",
    "            data['Season'] = season\n",
    "\n",
    "            all_data = pd.concat([all_data, data], ignore_index=True)\n",
    "\n",
    "            print(f\"Scraping data for season {season}\")\n",
    "\n",
    "    return all_data\n",
    "\n",
    "result = scrape_all_seasons()\n",
    "\n",
    "RS_Schedule_Team = result\n",
    "\n",
    "#save\n",
    "RS_Schedule_Team.to_csv('RS_Schedule_Team.csv', index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8ef012-2632-44dc-a872-4d0e8b206cdf",
   "metadata": {},
   "source": [
    "### PO | Schedule/Results | Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f8d788f5-6efc-45a5-ab3b-305c0d220c88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for season 2024\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def scrape_season(season):\n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    url = f'https://www.basketball-reference.com/playoffs/NBA_{season}_games.html'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        table = soup.find('table', {'id': 'schedule'})\n",
    "\n",
    "        if table:\n",
    "            html_str = str(table)\n",
    "            df = pd.read_html(StringIO(html_str))[0]\n",
    "\n",
    "            df['Season'] = season\n",
    "\n",
    "            all_data = pd.concat([all_data, df], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"No table found for {season}\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for {season}\")\n",
    "\n",
    "    time.sleep(4)\n",
    "\n",
    "    return all_data\n",
    "\n",
    "def scrape_all_seasons():\n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    for season in range(start, end):\n",
    "        data = scrape_season(str(season))\n",
    "\n",
    "        if data is not None:\n",
    "            data['Season'] = season\n",
    "\n",
    "            all_data = pd.concat([all_data, data], ignore_index=True)\n",
    "\n",
    "            print(f\"Scraping data for season {season}\")\n",
    "\n",
    "    return all_data\n",
    "\n",
    "result = scrape_all_seasons()\n",
    "\n",
    "PO_Schedule_Team = result\n",
    "PO_Schedule_Team.to_csv('PO_Schedule_Team.csv', index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "#=======================================================================================================\n",
    "\n",
    "#clean all schedule data\n",
    "\n",
    "dtype = {\n",
    "    'Date': str,\n",
    "    'Start (ET)': str,\n",
    "    'Visitor/Neutral': str,\n",
    "    'PTS': float,\n",
    "    'Home/Neutral': str,\n",
    "    'PTS.1': float,\n",
    "    'Unnamed: 7': str,  # Replace 'Unnamed: 7' with the actual column name if known\n",
    "    'Attend.': float,\n",
    "    'LOG': str,\n",
    "    'Notes': str\n",
    "}\n",
    "#btw play in games are NOT counted for both\n",
    "\n",
    "#=======================================================================================================\n",
    "\n",
    "#Cleaning regular season schedule data\n",
    "\n",
    "RS_Schedule_Team['matchup'] = RS_Schedule_Team['Visitor/Neutral'] + '_' + RS_Schedule_Team['Home/Neutral']\n",
    "\n",
    "RS_Schedule_Team['game_id'] = RS_Schedule_Team['Date'] + '_' + RS_Schedule_Team['matchup']\n",
    "\n",
    "RS_Schedule_Team = RS_Schedule_Team.loc[:, ~RS_Schedule_Team.columns.str.contains('^Unnamed')]\n",
    "\n",
    "RS_Schedule_Team = RS_Schedule_Team[~RS_Schedule_Team['Notes'].str.contains('Play-In Game', na=False)]\n",
    "\n",
    "columns_to_drop = ['Notes', 'Arena', 'LOG', 'Attend.']\n",
    "RS_Schedule_Team = RS_Schedule_Team.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "RS_Schedule_Team = RS_Schedule_Team.rename(columns={'PTS': 'v/n_pts', 'PTS.1': 'h/n_pts'})\n",
    "\n",
    "#=======================================================================================================\n",
    "\n",
    "#Cleaning playoff schedule data\n",
    "\n",
    "PO_Schedule_Team['matchup'] = PO_Schedule_Team['Visitor/Neutral'] + '_' + PO_Schedule_Team['Home/Neutral']\n",
    "\n",
    "PO_Schedule_Team['game_id'] = PO_Schedule_Team['Date'] + '_' + PO_Schedule_Team['matchup']\n",
    "\n",
    "PO_Schedule_Team = PO_Schedule_Team.loc[:, ~PO_Schedule_Team.columns.str.contains('^Unnamed')]\n",
    "\n",
    "columns_to_drop = ['Notes', 'Arena', 'LOG', 'Attend.']\n",
    "PO_Schedule_Team = PO_Schedule_Team.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "PO_Schedule_Team = PO_Schedule_Team.rename(columns={'PTS': 'v/n_pts', 'PTS.1': 'h/n_pts'})\n",
    "\n",
    "#=======================================================================================================\n",
    "\n",
    "RS_Schedule_Team = RS_Schedule_Team[~RS_Schedule_Team['game_id'].isin(PO_Schedule_Team['game_id'])]\n",
    "\n",
    "#save\n",
    "RS_Schedule_Team.to_csv('RS_Schedule_Team.csv', index=False, encoding=\"utf-8-sig\")\n",
    "PO_Schedule_Team.to_csv('PO_Schedule_Team.csv', index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22627b4a-f0b4-46bf-9bb8-343de124cf49",
   "metadata": {},
   "source": [
    "### Other | Pre-Season Odds | Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dadfb9f5-6bcb-4950-b1a8-4a54a1639452",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for season 2024\n"
     ]
    }
   ],
   "source": [
    "def scrape_season(season):\n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    url = f'https://www.basketball-reference.com/leagues/NBA_{season}_preseason_odds.html'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        table = soup.find('table', {'id': 'NBA_preseason_odds'})\n",
    "\n",
    "        if table:\n",
    "            html_str = str(table)\n",
    "            df = pd.read_html(StringIO(html_str))[0]\n",
    "\n",
    "            df['Season'] = season\n",
    "\n",
    "            all_data = pd.concat([all_data, df], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"No table found for {season}\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for {season}\")\n",
    "\n",
    "    time.sleep(4)\n",
    "\n",
    "    return all_data\n",
    "\n",
    "def scrape_all_seasons(seasons_list):\n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    for season in seasons_list:\n",
    "        data = scrape_season(season)\n",
    "\n",
    "        if data is not None:\n",
    "            data['Season'] = season\n",
    "\n",
    "            all_data = pd.concat([all_data, data], ignore_index=True)\n",
    "\n",
    "            print(f\"Scraping data for season {season}\")\n",
    "\n",
    "    return all_data\n",
    "\n",
    "seasons_list = [str(year) for year in range(start, end)]\n",
    "\n",
    "result = scrape_all_seasons(seasons_list)\n",
    "\n",
    "PSO_Team = result\n",
    "\n",
    "#=================================================================================================\n",
    "\n",
    "#cleaning pso team data\n",
    "\n",
    "PSO_Team = PSO_Team.loc[:, ~PSO_Team.columns.str.contains('^Unnamed')]\n",
    "\n",
    "PSO_Team['overall_record_o/u'] = PSO_Team['Result'].str.contains(r'\\(over\\)', na=False).astype(int)\n",
    "\n",
    "PSO_Team = PSO_Team.rename(columns={'W-L O/U': 'projected_overall_record'})\n",
    "\n",
    "def calculate_overall_record(result):\n",
    "    if pd.isna(result):\n",
    "        return None\n",
    "    try:\n",
    "        record = result.split(' ')[0]  #rec part\n",
    "        wins, losses = map(int, record.split('-'))\n",
    "        return wins / (wins + losses)\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "PSO_Team['overall_record'] = PSO_Team['Result'].apply(calculate_overall_record)\n",
    "\n",
    "PSO_Team['overall_record'] = (PSO_Team['overall_record'] * 100).round(1)\n",
    "\n",
    "PSO_Team = PSO_Team.drop(columns=['Result'])\n",
    "\n",
    "#save\n",
    "PSO_Team.to_csv('PSO_Team.csv', index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aef256-2699-497e-b33e-0845118b0a21",
   "metadata": {},
   "source": [
    "### RS | Expanded Standings | Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d77d3525-295d-4f71-a4ab-2f6d91e61ed1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for season 1950\n",
      "Scraping data for season 1951\n",
      "Scraping data for season 1952\n",
      "Scraping data for season 1953\n",
      "Scraping data for season 1954\n",
      "Scraping data for season 1955\n",
      "Scraping data for season 1956\n",
      "Scraping data for season 1957\n",
      "Scraping data for season 1958\n",
      "Scraping data for season 1959\n",
      "Scraping data for season 1960\n",
      "Scraping data for season 1961\n",
      "Scraping data for season 1962\n",
      "Scraping data for season 1963\n",
      "Scraping data for season 1964\n",
      "Scraping data for season 1965\n",
      "Scraping data for season 1966\n",
      "Scraping data for season 1967\n",
      "Scraping data for season 1968\n",
      "Scraping data for season 1969\n",
      "Scraping data for season 1970\n",
      "Scraping data for season 1971\n",
      "Scraping data for season 1972\n",
      "Scraping data for season 1973\n",
      "Scraping data for season 1974\n",
      "Scraping data for season 1975\n",
      "Scraping data for season 1976\n",
      "Scraping data for season 1977\n",
      "Scraping data for season 1978\n",
      "Scraping data for season 1979\n",
      "Scraping data for season 1980\n",
      "Scraping data for season 1981\n",
      "Scraping data for season 1982\n",
      "Scraping data for season 1983\n",
      "Scraping data for season 1984\n",
      "Scraping data for season 1985\n",
      "Scraping data for season 1986\n",
      "Scraping data for season 1987\n",
      "Scraping data for season 1988\n",
      "Scraping data for season 1989\n",
      "Scraping data for season 1990\n",
      "Scraping data for season 1991\n",
      "Scraping data for season 1992\n",
      "Scraping data for season 1993\n",
      "Scraping data for season 1994\n",
      "Scraping data for season 1995\n",
      "Scraping data for season 1996\n",
      "Scraping data for season 1997\n",
      "Scraping data for season 1998\n",
      "Scraping data for season 1999\n",
      "Scraping data for season 2000\n",
      "Scraping data for season 2001\n",
      "Scraping data for season 2002\n",
      "Scraping data for season 2003\n",
      "Scraping data for season 2004\n",
      "Scraping data for season 2005\n",
      "Scraping data for season 2006\n",
      "Scraping data for season 2007\n",
      "Scraping data for season 2008\n",
      "Scraping data for season 2009\n",
      "Scraping data for season 2010\n",
      "Scraping data for season 2011\n",
      "Scraping data for season 2012\n",
      "Scraping data for season 2013\n",
      "Scraping data for season 2014\n",
      "Scraping data for season 2015\n",
      "Scraping data for season 2016\n",
      "Scraping data for season 2017\n",
      "Scraping data for season 2018\n",
      "Scraping data for season 2019\n",
      "Scraping data for season 2020\n",
      "Scraping data for season 2021\n",
      "Scraping data for season 2022\n",
      "Scraping data for season 2023\n",
      "Scraping data for season 2024\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def scrape_season(season):\n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    url = f'https://www.basketball-reference.com/leagues/NBA_{season}_standings.html#all_expanded_standings'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
    "        \n",
    "        for comment in comments:\n",
    "            if 'expanded_standings' in comment:\n",
    "                comment_soup = BeautifulSoup(comment, 'html.parser')\n",
    "                table = comment_soup.find('table', {'id': 'expanded_standings'})\n",
    "                \n",
    "                if table:\n",
    "                    \n",
    "                    df = pd.read_html(StringIO(str(table)))[0]\n",
    "\n",
    "                    df['Season'] = season\n",
    "\n",
    "                    all_data = pd.concat([all_data, df], ignore_index=True)\n",
    "                    break\n",
    "        else:\n",
    "            print(f\"No table found for {season}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for {season}\")\n",
    "        return None\n",
    "\n",
    "    time.sleep(4)\n",
    "\n",
    "    return all_data\n",
    "\n",
    "def scrape_all_seasons():\n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    for season in range(start, end):\n",
    "        data = scrape_season(str(season))\n",
    "\n",
    "        if data is not None:\n",
    "            data['Season'] = season\n",
    "\n",
    "            all_data = pd.concat([all_data, data], ignore_index=True)\n",
    "\n",
    "            print(f\"Scraping data for season {season}\")\n",
    "\n",
    "    return all_data\n",
    "\n",
    "result = scrape_all_seasons()\n",
    "\n",
    "RS_exp_stand_Team = result\n",
    "\n",
    "#==========================================================================================================================\n",
    "\n",
    "#clean expanded standings data\n",
    "\n",
    "RS_exp_stand_Team = pd.read_csv(\"RS_exp_stand_Team.csv\")\n",
    "\n",
    "RS_exp_stand_Team.columns = RS_exp_stand_Team.iloc[0]\n",
    "RS_exp_stand_Team = RS_exp_stand_Team[1:]\n",
    "\n",
    "RS_exp_stand_Team.reset_index(drop=True, inplace=True)\n",
    "\n",
    "new_column_headers = [\n",
    "    'Rk', 'Team', 'overall_record', 'home_rec', 'road_rec', 'neu_rec', 'cen_div_rec', 'e_div_rec',\n",
    "    'w_div_rec', '3pt_or_less_rec', '10pt_or_more_rec', 'oct_mon_rec', 'nov_mon_rec', 'dec_mon_rec',\n",
    "    'jan_mon_rec', 'feb_mon_rec', 'mar_mon_rec', 'season', 'pre_all_star_rec', 'post_all_star_rec',\n",
    "    'e_conf_rec', 'w_conf_rec', 'atl_div_rec', 'midw_div_rec', 'pac_div_rec', 'apr_mon_rec',\n",
    "    'may_mon_rec', 'se_div_rec', 'nw_div_rec', 'sw_div_rec', 'jul_mon_rec', 'aug_mon_rec'\n",
    "]\n",
    "\n",
    "if len(RS_exp_stand_Team.columns) >= len(new_column_headers):\n",
    "    RS_exp_stand_Team.columns = new_column_headers\n",
    "else:\n",
    "    print(\"The DataFrame does not have enough columns to rename according to the provided headers.\")\n",
    "\n",
    "def calculate_winning_percentage(record):\n",
    "    try:\n",
    "        wins, losses = map(int, record.split('-'))\n",
    "        percentage = wins / (wins + losses)\n",
    "        return percentage\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "for column in RS_exp_stand_Team.columns:\n",
    "    if column not in ['Rk', 'Team', 'season']:\n",
    "        RS_exp_stand_Team[column] = RS_exp_stand_Team[column].apply(calculate_winning_percentage)\n",
    "\n",
    "\n",
    "#save\n",
    "RS_exp_stand_Team.to_csv('RS_exp_stand_Team.csv', index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a57e04-eaa9-4f9b-a3c0-f6c7ece04df7",
   "metadata": {},
   "source": [
    "### Season | Coaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a5c37353-85de-46d4-9332-0358025b6b72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for season 2024\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def scrape_season(season):\n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    url = f'https://www.basketball-reference.com/leagues/NBA_{season}_coaches.html'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        table = soup.find('table', {'id': 'NBA_coaches'})\n",
    "\n",
    "        if table:\n",
    "            df = pd.read_html(io.StringIO(str(table)))[0]\n",
    "\n",
    "            df['Season'] = season\n",
    "\n",
    "            all_data = pd.concat([all_data, df], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"No table found for {season}\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for {season}\")\n",
    "\n",
    "    time.sleep(4)\n",
    "\n",
    "    return all_data\n",
    "\n",
    "def scrape_all_seasons():\n",
    "    all_data = pd.DataFrame()\n",
    "\n",
    "    for season in range(start, end):  \n",
    "        data = scrape_season(str(season))\n",
    "\n",
    "        if data is not None:\n",
    "            data['Season'] = season\n",
    "            all_data = pd.concat([all_data, data], ignore_index=True)\n",
    "\n",
    "            print(f\"Scraping data for season {season}\")\n",
    "\n",
    "    return all_data\n",
    "\n",
    "result = scrape_all_seasons()\n",
    "\n",
    "coaches_season = result\n",
    "\n",
    "#==============================================================================================================\n",
    "\n",
    "#clean coach data\n",
    "\n",
    "coaches_season = coaches_season.iloc[2:].reset_index(drop=True)\n",
    "\n",
    "columns_to_drop = [2, 5, 16]\n",
    "coaches_season.drop(coaches_season.columns[columns_to_drop], axis=1, inplace=True)\n",
    "\n",
    "new_column_headers = [\n",
    "    'Coach', 'Tm', 'sea_w/franc_count', 'sea_overall_count', 'rs_curr_sea_G', 'rs_curr_sea_W', 'rs_curr_sea_L',\n",
    "    'rs_w/franc_G', 'rs_w/franc_W', 'rs_w/franc_L', 'rs_career_G', 'rs_career_W',\n",
    "    'rs_career_L', 'rs_career_W%', 'po_curr_sea_G', 'po_curr_sea_W', 'po_curr_sea_L',\n",
    "    'po_w/franc_G', 'po_w/franc_W', 'po_w/franc_L', 'po_career_G', 'po_career_W', 'po_career_L', 'season'\n",
    "]\n",
    "\n",
    "coaches_season.columns = new_column_headers\n",
    "\n",
    "coaches_season['season'] = coaches_season['season'].astype(float).round().astype(int)\n",
    "\n",
    "#save\n",
    "coaches_season.to_csv('coaches_season.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a9dc60-7ce0-49df-85c8-74f008083f54",
   "metadata": {},
   "source": [
    "### Final cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "387a5fa2-8de3-4171-b356-b631c40772e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#cleaned files\n",
    "PO_Schedule_Team = pd.read_csv(\"PO_Schedule_Team.csv\")\n",
    "RS_Schedule_Team = pd.read_csv(\"RS_Schedule_Team.csv\")\n",
    "\n",
    "custom_team_season_index = pd.read_csv(\"custom_team_season_index.csv\")\n",
    "custom_team_franchise_index = pd.read_csv(\"custom_team_franchise_index.csv\")\n",
    "\n",
    "#clean up files\n",
    "coaches_season = pd.read_csv(\"coaches_season.csv\")\n",
    "RS_exp_stand_Team = pd.read_csv(\"RS_exp_stand_Team.csv\") \n",
    "PSO_Team = pd.read_csv(\"PSO_Team.csv\")\n",
    "\n",
    "PO_Advanced_Team = pd.read_csv(\"PO_Advanced_Team.csv\")\n",
    "RS_Advanced_Team = pd.read_csv(\"RS_Advanced_Team.csv\")\n",
    "RS_Opp_Per_Game_Team = pd.read_csv(\"RS_Opp_Per_Game_Team.csv\") \n",
    "RS_Per_Game_Team = pd.read_csv(\"RS_Per_Game_Team.csv\")\n",
    "\n",
    "all_award_coach_voting = pd.read_csv(\"all_award_coach_voting.csv\") \n",
    "all_award_voting = pd.read_csv(\"all_award_voting.csv\")\n",
    "\n",
    "PO_Totals_Player = pd.read_csv(\"PO_Totals_Player.csv\")\n",
    "PO_Advanced_Player = pd.read_csv(\"PO_Advanced_Player.csv\")\n",
    "PO_Per_Game_Player = pd.read_csv(\"PO_Per_Game_Player.csv\")\n",
    "\n",
    "RS_Totals_Player = pd.read_csv(\"RS_Totals_Player.csv\")\n",
    "RS_Advanced_Player = pd.read_csv(\"RS_Advanced_Player.csv\") \n",
    "RS_Per_Game_Player = pd.read_csv(\"RS_Per_Game_Player.csv\")\n",
    "\n",
    "RS_exp_stand_Team = pd.read_csv(\"RS_exp_stand_Team.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3341651-b3e8-41e8-b26e-0d0b36cb9bcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#cleaning and merging, add index columns to datasets\n",
    "\n",
    "#giving these dfs the team_id column from the index\n",
    "coaches_season = pd.merge(coaches_season, custom_team_franchise_index[['Tm', 'team_id']], on='Tm', how='left')\n",
    "RS_exp_stand_Team = pd.merge(RS_exp_stand_Team, custom_team_franchise_index[['Team', 'team_id']], on='Team', how='left')\n",
    "PSO_Team = pd.merge(PSO_Team, custom_team_franchise_index[['Team', 'team_id']], on='Team', how='left')\n",
    "all_award_coach_voting = pd.merge(all_award_coach_voting, custom_team_franchise_index[['Tm', 'team_id']], on='Tm', how='left')\n",
    "\n",
    "#=================================================================================================================================\n",
    "\n",
    "#remove \"*\" from all rows in the 'Player' column\n",
    "all_award_voting['Player'] = all_award_voting['Player'].str.replace('*', '')\n",
    "\n",
    "#drop all rows that contain \"Player\" in the 'Player' column\n",
    "all_award_voting = all_award_voting[all_award_voting['Player'] != 'Player']\n",
    "\n",
    "#=================================================================================================================================\n",
    "\n",
    "#clean po advanced team \n",
    "\n",
    "#if there is a NA in column Team fill that with value/text in column Tm\n",
    "PO_Advanced_Team['Team'] = PO_Advanced_Team['Team'].fillna(PO_Advanced_Team['Tm'])\n",
    "\n",
    "#then delete column Tm\n",
    "PO_Advanced_Team.drop(columns=['Tm'], inplace=True)\n",
    "\n",
    "#delete row(s) in 'League Average' in column Team\n",
    "PO_Advanced_Team = PO_Advanced_Team[PO_Advanced_Team['Team'] != 'League Average']\n",
    "\n",
    "#give team_id column to po advanced team\n",
    "PO_Advanced_Team = pd.merge(PO_Advanced_Team, custom_team_franchise_index[['Team', 'team_id']], on='Team', how='left')\n",
    "\n",
    "#make sure team_id is a whole number in dataframe\n",
    "PO_Advanced_Team['team_id'] = PO_Advanced_Team['team_id'].round().astype(int)\n",
    "\n",
    "#add new feature champion, if champion share == 1 then give it 1\n",
    "PO_Advanced_Team['champion'] = PO_Advanced_Team['champion_share'].apply(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "#=================================================================================================================================\n",
    "\n",
    "#clean rs advanced team \n",
    "\n",
    "#give rs advanced team the team_id column\n",
    "RS_Advanced_Team = pd.merge(RS_Advanced_Team, custom_team_franchise_index[['Team', 'team_id']], on='Team', how='left')\n",
    "\n",
    "#=================================================================================================================================\n",
    "\n",
    "#clean rs opp per game team\n",
    "\n",
    "#give rs opp per game team the team id column\n",
    "RS_Opp_Per_Game_Team = pd.merge(RS_Opp_Per_Game_Team, custom_team_franchise_index[['Team', 'team_id']], on='Team', how='left')\n",
    "\n",
    "\n",
    "#clean rs per game team\n",
    "\n",
    "#=================================================================================================================================\n",
    "\n",
    "#give rs per game the team id column\n",
    "RS_Per_Game_Team = pd.merge(RS_Per_Game_Team, custom_team_franchise_index[['Team', 'team_id']], on='Team', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb61af78-cbb0-4325-9bab-0a109e90dbec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#function to rename any column called 'Season' to 'season in our dfs for consistency\n",
    "def rename_season_column(df):\n",
    "    if 'Season' in df.columns:\n",
    "        df.rename(columns={'Season': 'season'}, inplace=True)\n",
    "    elif 'season' in df.columns:\n",
    "        df.rename(columns={'season': 'season'}, inplace=True)\n",
    "    return df\n",
    "\n",
    "#dfs to process\n",
    "dfs = {\n",
    "    'coaches_season': coaches_season,\n",
    "    'RS_exp_stand_Team': RS_exp_stand_Team,\n",
    "    'PSO_Team': PSO_Team,\n",
    "    'all_award_coach_voting': all_award_coach_voting,\n",
    "    'all_award_voting': all_award_voting,\n",
    "    'PO_Totals_Player': PO_Totals_Player,\n",
    "    'PO_Advanced_Player': PO_Advanced_Player,\n",
    "    'PO_Per_Game_Player': PO_Per_Game_Player,\n",
    "    'PO_Advanced_Team': PO_Advanced_Team,\n",
    "    'RS_Advanced_Team': RS_Advanced_Team,\n",
    "    'RS_Opp_Per_Game_Team': RS_Opp_Per_Game_Team,\n",
    "    'RS_Per_Game_Team': RS_Per_Game_Team,\n",
    "    'RS_Totals_Player': RS_Totals_Player,\n",
    "    'RS_Advanced_Player': RS_Advanced_Player,\n",
    "    'RS_Per_Game_Player': RS_Per_Game_Player\n",
    "}\n",
    "\n",
    "#apply\n",
    "for name, df in dfs.items():\n",
    "    dfs[name] = rename_season_column(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5108d19b-42d9-4a9b-8ee9-2af5c203ead1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_teams_and_filter(df, po_df):\n",
    "    #make sure all 'season' column in those dfs are rounded to whole value\n",
    "    df['season'] = df['season'].astype(int)\n",
    "    po_df['season'] = po_df['season'].astype(int)\n",
    "    \n",
    "    #merging to to get the Tm_po column from po per game player df so we have a column indicating if were that players is playing on team come playoff time\n",
    "    merged_df = df.merge( \n",
    "        po_df[['Player', 'season', 'Tm']], \n",
    "        on=['Player', 'season'], \n",
    "        how='left',\n",
    "        suffixes=('', '_po')\n",
    "    )\n",
    "\n",
    "    #create team update column with the values from Tm_PO or fall back to the original values in Tm\n",
    "    merged_df['team_update'] = merged_df['Tm_po'].combine_first(merged_df['Tm'])\n",
    "\n",
    "    #drop the extra Tm po column used for merging\n",
    "    merged_df.drop(columns=['Tm_po'], inplace=True)\n",
    "\n",
    "    #rename the 'Tm' column to 'team_before_td'\n",
    "    merged_df.rename(columns={'Tm': 'team_before_td'}, inplace=True)\n",
    "\n",
    "    #group by 'Player' and 'season', and select the row with the maximum value in 'G'\n",
    "    filtered_df = merged_df.loc[merged_df.groupby(['Player', 'season'])['G'].idxmax()]\n",
    "\n",
    "    #create the 'team_after_td' column based on the specified criteria\n",
    "    filtered_df['team_after_td'] = filtered_df.apply(\n",
    "        lambda row: row['team_update'] if row['team_before_td'] == 'TOT' else row['team_before_td'], axis=1\n",
    "    )\n",
    "\n",
    "    #drop the team update column\n",
    "    filtered_df.drop(columns=['team_update'], inplace=True)\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "#apply function to these dfs\n",
    "RS_Per_Game_Player = update_teams_and_filter(RS_Per_Game_Player, PO_Per_Game_Player)\n",
    "RS_Advanced_Player = update_teams_and_filter(RS_Advanced_Player, PO_Per_Game_Player)\n",
    "RS_Totals_Player = update_teams_and_filter(RS_Totals_Player, PO_Per_Game_Player)\n",
    "\n",
    "#============================================================================================================================\n",
    "\n",
    "#cleaning the award data\n",
    "\n",
    "#list of columns to average- JA\n",
    "columns_to_average = [\n",
    "    '1st_team', '2nd_team', '3rd_team', 'mvp_share', 'dpoy_share', 'roy_share', 'smoy_share', \n",
    "    'mip_share', 'cpoy_share', 'leading_all_nba_1st_team', 'leading_all_nba_2nd_team', \n",
    "    'leading_all_nba_3rd_team', 'leading_all_defense_1st_team', 'leading_all_defense_2nd_team', \n",
    "    'leading_all_rookie_1st_team', 'leading_all_rookie_2nd_team', 'count_all_nba', \n",
    "    'count_all_defense', 'count_all_rookie', 'won_mvp', 'won_roy', 'won_dpoy', \n",
    "    'won_smoy', 'won_mip', 'won_cpoy'\n",
    "]\n",
    "\n",
    "#group by 'Player' and 'season' and calculate the mean of the specified columns- JA\n",
    "all_award_voting = all_award_voting.groupby(['Player', 'season'])[columns_to_average].mean().reset_index()\n",
    "\n",
    "#columns to check and convert values greater than 0 to 1- JA \n",
    "columns_to_convert = [\n",
    "    '1st_team', '2nd_team', '3rd_team', 'leading_all_nba_1st_team', 'leading_all_nba_2nd_team', \n",
    "    'leading_all_nba_3rd_team', 'leading_all_defense_1st_team', 'leading_all_defense_2nd_team', \n",
    "    'leading_all_rookie_1st_team', 'leading_all_rookie_2nd_team', 'count_all_nba', \n",
    "    'count_all_defense', 'count_all_rookie', 'won_mvp', 'won_roy', 'won_dpoy', \n",
    "    'won_smoy', 'won_mip', 'won_cpoy'\n",
    "]\n",
    "\n",
    "#convert values greater than 0 to 1- JA\n",
    "for col in columns_to_convert:\n",
    "    all_award_voting[col] = all_award_voting[col].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c07fce-a3f8-426d-a071-d8e1966bd660",
   "metadata": {},
   "source": [
    "#### Final raw data csv's | Cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0652385e-5dac-4c17-be97-3aa861b4b929",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coaches_season.to_csv('coaches_season.csv', index=False, encoding=\"utf-8-sig\")\n",
    "RS_exp_stand_Team.to_csv('RS_exp_stand_team.csv', index=False, encoding=\"utf-8-sig\")\n",
    "PSO_Team.to_csv('PSO_Team.csv', index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "all_award_coach_voting.to_csv('all_award_coach_voting.csv', index=False, encoding=\"utf-8-sig\")\n",
    "all_award_voting.to_csv('all_award_voting.csv', index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "PO_Totals_Player.to_csv('PO_Totals_Player.csv', index=False, encoding=\"utf-8-sig\")\n",
    "PO_Advanced_Player.to_csv('PO_Advanced_Player.csv', index=False, encoding=\"utf-8-sig\")\n",
    "PO_Per_Game_Player.to_csv('PO_Per_Game_Player.csv', index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "PO_Advanced_Team.to_csv('PO_Advanced_Team.csv', index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "RS_Advanced_Team.to_csv('RS_Advanced_Team.csv', index=False, encoding=\"utf-8-sig\")\n",
    "RS_Opp_Per_Game_Team.to_csv('RS_Opp_Per_Game_Team.csv', index=False, encoding=\"utf-8-sig\")\n",
    "RS_Per_Game_Team.to_csv('RS_Per_Game_Team.csv', index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "RS_Totals_Player.to_csv('RS_Totals_Player.csv', index=False, encoding=\"utf-8-sig\")\n",
    "RS_Advanced_Player.to_csv('RS_Advanced_Player.csv', index=False, encoding=\"utf-8-sig\")\n",
    "RS_Per_Game_Player.to_csv('RS_Per_Game_Player.csv', index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76275120-a502-41df-a08a-1c19936eb91f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
